name: QA Testing Pipeline

on:
  push:
    branches: [ main, master, develop ]
    paths:
      - 'src/gov_scraper/processors/qa.py'
      - 'tests/qa/**'
      - 'bin/qa.py'
      - 'requirements.txt'
      - '.github/workflows/qa-tests.yml'
  pull_request:
    branches: [ main, master, develop ]
    paths:
      - 'src/gov_scraper/processors/qa.py'
      - 'tests/qa/**'
      - 'bin/qa.py'
      - 'requirements.txt'
  schedule:
    # Run nightly at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      run_performance_tests:
        description: 'Run performance tests'
        required: false
        default: false
        type: boolean
      run_property_tests:
        description: 'Run property-based tests'
        required: false
        default: true
        type: boolean

env:
  PYTHON_VERSION: '3.11'
  CACHE_VERSION: 1

jobs:
  unit-tests:
    name: ðŸ§ª Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
    - name: ðŸ“¥ Checkout code
      uses: actions/checkout@v4

    - name: ðŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: ðŸ“¦ Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-
          ${{ runner.os }}-pip-

    - name: ðŸ”§ Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-html pytest-xdist hypothesis

    - name: ðŸ§ª Run unit tests
      run: |
        python -m pytest tests/qa/unit/ \
          --verbose \
          --cov=src/gov_scraper/processors/qa \
          --cov-report=xml:coverage-unit.xml \
          --cov-report=html:htmlcov-unit \
          --junitxml=test-results-unit.xml \
          --tb=short

    - name: ðŸ“Š Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage-unit.xml
        flags: unit-tests
        name: Unit Tests Coverage

    - name: ðŸ“„ Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: unit-test-results
        path: |
          test-results-unit.xml
          htmlcov-unit/
        retention-days: 7

  integration-tests:
    name: ðŸ”— Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: unit-tests

    steps:
    - name: ðŸ“¥ Checkout code
      uses: actions/checkout@v4

    - name: ðŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: ðŸ“¦ Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-${{ hashFiles('requirements.txt') }}

    - name: ðŸ”§ Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-html pytest-xdist hypothesis

    - name: ðŸ”— Run integration tests
      run: |
        python -m pytest tests/qa/integration/ \
          --verbose \
          --cov=src/gov_scraper/processors/qa \
          --cov-report=xml:coverage-integration.xml \
          --cov-report=html:htmlcov-integration \
          --junitxml=test-results-integration.xml \
          --tb=short

    - name: ðŸ“Š Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage-integration.xml
        flags: integration-tests
        name: Integration Tests Coverage

    - name: ðŸ“„ Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: integration-test-results
        path: |
          test-results-integration.xml
          htmlcov-integration/
        retention-days: 7

  regression-tests:
    name: ðŸ”„ Regression Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: unit-tests

    steps:
    - name: ðŸ“¥ Checkout code
      uses: actions/checkout@v4

    - name: ðŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: ðŸ“¦ Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-${{ hashFiles('requirements.txt') }}

    - name: ðŸ”§ Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-html hypothesis

    - name: ðŸ”„ Run regression tests
      run: |
        python -m pytest tests/qa/regression/ \
          --verbose \
          --cov=src/gov_scraper/processors/qa \
          --cov-report=xml:coverage-regression.xml \
          --cov-report=html:htmlcov-regression \
          --junitxml=test-results-regression.xml \
          --tb=short \
          -m regression

    - name: ðŸ“Š Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage-regression.xml
        flags: regression-tests
        name: Regression Tests Coverage

    - name: ðŸ“„ Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: regression-test-results
        path: |
          test-results-regression.xml
          htmlcov-regression/
        retention-days: 30  # Keep regression results longer

  property-tests:
    name: ðŸŽ¯ Property Tests
    runs-on: ubuntu-latest
    timeout-minutes: 25
    needs: unit-tests
    if: ${{ github.event.inputs.run_property_tests == 'true' || github.event_name == 'schedule' || github.event_name == 'push' }}

    steps:
    - name: ðŸ“¥ Checkout code
      uses: actions/checkout@v4

    - name: ðŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: ðŸ“¦ Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-${{ hashFiles('requirements.txt') }}

    - name: ðŸ”§ Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov hypothesis

    - name: ðŸŽ¯ Run property-based tests
      run: |
        python -m pytest tests/qa/property/ \
          --verbose \
          --cov=src/gov_scraper/processors/qa \
          --cov-report=xml:coverage-property.xml \
          --cov-report=html:htmlcov-property \
          --junitxml=test-results-property.xml \
          --tb=short \
          -m property

    - name: ðŸ“Š Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage-property.xml
        flags: property-tests
        name: Property Tests Coverage

    - name: ðŸ“„ Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: property-test-results
        path: |
          test-results-property.xml
          htmlcov-property/
        retention-days: 7

  performance-tests:
    name: âš¡ Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [unit-tests, integration-tests]
    if: ${{ github.event.inputs.run_performance_tests == 'true' || github.event_name == 'schedule' }}

    steps:
    - name: ðŸ“¥ Checkout code
      uses: actions/checkout@v4

    - name: ðŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: ðŸ“¦ Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-${{ hashFiles('requirements.txt') }}

    - name: ðŸ”§ Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-benchmark psutil

    - name: âš¡ Run performance tests
      run: |
        python -m pytest tests/qa/performance/ \
          --verbose \
          --cov=src/gov_scraper/processors/qa \
          --cov-report=xml:coverage-performance.xml \
          --junitxml=test-results-performance.xml \
          --tb=short \
          -m performance \
          --benchmark-only

    - name: ðŸ“Š Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage-performance.xml
        flags: performance-tests
        name: Performance Tests Coverage

    - name: ðŸ“„ Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-test-results
        path: |
          test-results-performance.xml
        retention-days: 30

    - name: ðŸ“ˆ Performance regression check
      run: |
        echo "Performance tests completed. Check results for regressions."
        # TODO: Add performance regression detection logic

  code-quality:
    name: ðŸ” Code Quality
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
    - name: ðŸ“¥ Checkout code
      uses: actions/checkout@v4

    - name: ðŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: ðŸ”§ Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 black isort mypy

    - name: ðŸŽ¨ Check code formatting (Black)
      run: black --check --diff src/gov_scraper/processors/qa.py tests/qa/

    - name: ðŸ“¦ Check import sorting (isort)
      run: isort --check-only --diff src/gov_scraper/processors/qa.py tests/qa/

    - name: ðŸ” Lint with flake8
      run: |
        flake8 src/gov_scraper/processors/qa.py tests/qa/ \
          --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 src/gov_scraper/processors/qa.py tests/qa/ \
          --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

  security-scan:
    name: ðŸ”’ Security Scan
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
    - name: ðŸ“¥ Checkout code
      uses: actions/checkout@v4

    - name: ðŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: ðŸ”§ Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install bandit safety

    - name: ðŸ”’ Run Bandit security scan
      run: |
        bandit -r src/gov_scraper/processors/qa.py -f json -o bandit-report.json || true
        bandit -r src/gov_scraper/processors/qa.py

    - name: ðŸ›¡ï¸ Check dependencies for vulnerabilities
      run: |
        pip install -r requirements.txt
        safety check

    - name: ðŸ“„ Upload security report
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-report
        path: bandit-report.json
        retention-days: 30

  test-summary:
    name: ðŸ“‹ Test Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, regression-tests, code-quality]
    if: always()

    steps:
    - name: ðŸ“¥ Download all test results
      uses: actions/download-artifact@v3

    - name: ðŸ“Š Generate test summary
      run: |
        echo "# ðŸ§ª QA Test Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Test Results" >> $GITHUB_STEP_SUMMARY

        # Check job statuses
        if [ "${{ needs.unit-tests.result }}" == "success" ]; then
          echo "âœ… **Unit Tests**: Passed" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **Unit Tests**: Failed" >> $GITHUB_STEP_SUMMARY
        fi

        if [ "${{ needs.integration-tests.result }}" == "success" ]; then
          echo "âœ… **Integration Tests**: Passed" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **Integration Tests**: Failed" >> $GITHUB_STEP_SUMMARY
        fi

        if [ "${{ needs.regression-tests.result }}" == "success" ]; then
          echo "âœ… **Regression Tests**: Passed" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **Regression Tests**: Failed" >> $GITHUB_STEP_SUMMARY
        fi

        if [ "${{ needs.code-quality.result }}" == "success" ]; then
          echo "âœ… **Code Quality**: Passed" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **Code Quality**: Failed" >> $GITHUB_STEP_SUMMARY
        fi

        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## ðŸ“Š Coverage Reports" >> $GITHUB_STEP_SUMMARY
        echo "Coverage reports are available in the job artifacts." >> $GITHUB_STEP_SUMMARY

        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## â±ï¸ Workflow Duration" >> $GITHUB_STEP_SUMMARY
        echo "Started: $(date -u)" >> $GITHUB_STEP_SUMMARY

  notify:
    name: ðŸ“§ Notify
    runs-on: ubuntu-latest
    needs: [test-summary]
    if: failure() && github.event_name == 'schedule'

    steps:
    - name: ðŸ“§ Notify on failure (scheduled runs)
      run: |
        echo "Scheduled QA tests failed. Consider investigating."
        # TODO: Add notification logic (Slack, email, etc.)

# Separate workflow for deployment gates
  deployment-gate:
    name: ðŸšª Deployment Gate
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, regression-tests, code-quality, security-scan]
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master')

    steps:
    - name: âœ… All checks passed
      if: ${{ needs.unit-tests.result == 'success' && needs.integration-tests.result == 'success' && needs.regression-tests.result == 'success' && needs.code-quality.result == 'success' && needs.security-scan.result == 'success' }}
      run: |
        echo "ðŸŽ‰ All QA checks passed! Ready for deployment."
        echo "DEPLOYMENT_READY=true" >> $GITHUB_ENV

    - name: âŒ Some checks failed
      if: ${{ needs.unit-tests.result != 'success' || needs.integration-tests.result != 'success' || needs.regression-tests.result != 'success' || needs.code-quality.result != 'success' || needs.security-scan.result != 'success' }}
      run: |
        echo "âŒ Some QA checks failed. Deployment blocked."
        echo "DEPLOYMENT_READY=false" >> $GITHUB_ENV
        exit 1

    outputs:
      deployment_ready: ${{ env.DEPLOYMENT_READY }}