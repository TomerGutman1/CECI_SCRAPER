# QA Lessons Learned — GOV2DB

מסמך לקחים ממהלך ה-QA על ~25,000 החלטות ממשלה.
נועד לשמש בסיס לשיפור מתמיד של הפייפליין ולהפיכת תהליך ה-QA לאוטומטי לחלוטין.

---

## 1. בעיות שגילינו

### 1.1 תוכן פגום — Cloudflare Challenge Pages (חומרה: קריטית) ✅ תוקן

**מה קרה:** הסקרייפר (Selenium) נחסם לפעמים ע"י Cloudflare ושמר את דף אימות הבוט ("Just a moment...Verify you are human...Cloudflare") בשדה `decision_content` במקום תוכן ההחלטה האמיתי.

**היקף:** 24 רשומות מתוך 25,019 (0.1%) — מספר קטן אבל קריטי כי כל שדות ה-AI (תקציר, תיוגים, אופרטיביות) מבוססים על תוכן זבל.

**השפעה המשנית:** רשומות אלה גרמו ל-false positives בכל הבדיקות שמסתמכות על תוכן — `title_vs_content`, `policy_tag_relevance`, `government_body_hallucination` — כי שום מילה מהתוכן האמיתי לא קיימת.

**פתרון שיושם:**
- הוספת זיהוי Cloudflare ב-`check_content_quality` (חומרה HIGH)
- סינון Cloudflare content בכל הסורקים (title_vs_content, policy_tag_relevance, gov_body_hallucination)
- ✅ **Re-scrape (31.01.2026):** `fix_cloudflare` — 24/24 רשומות נסרקו מחדש בהצלחה, כל שדות ה-AI נוצרו מחדש (0 שגיאות). **תוצאה:** cloudflare_pages ירד מ-24 ל-**0**.

**לקח לפייפליין:**
- צריך לוודא שהתוכן שנסרק הוא אכן תוכן ההחלטה ולא דף אימות/שגיאה
- הוספת בדיקה ב-`scrape_decision_with_url_recovery()`: אם התוכן מכיל "Cloudflare" או "Just a moment", לנסות שוב או לדווח כשגיאה
- שקילת retry אוטומטי עם המתנה ארוכה יותר

### 1.2 תוכן קצר מדי (חומרה: גבוהה)

**מה קרה:** 499 רשומות (2%) עם תוכן קצר מ-100 תווים. חלק מהן ייתכן שנסרקו כשהדף טרם טען, חלק אחר הן החלטות שבאמת קצרות.

**לקח:** הסקרייפר צריך בדיקת סף מינימלי של תוכן. אם התוכן קצר מ-100 תווים, כדאי לנסות שוב.

### 1.3 טקסט ניווט בתוכן (חומרה: נמוכה)

**מה קרה:** 2 רשומות מכילות "דלג לתוכן האתר" — הסקרייפר לקח את ה-HTML הגולמי כולל אלמנטי ניווט.

**לקח:** צריך לוודא שהחילוץ מתמקד באלמנט התוכן הנכון (CSS selector ספציפי) ולא בכל הדף.

---

### 1.4 הטיית אופרטיביות — Operativity Bias (חומרה: גבוהה) ✅ תוקן

**מה קרה:** 84% מההחלטות סווגו כ"אופרטיבית" — הטיה ברורה. הצפי הריאלי הוא 50-65%.

**שורשי הבעיה (3 גורמים):**
1. **פרומפט מעורפל** — בגרסה הישנה של `generate_operativity()` לא היו דוגמאות (few-shot examples) ולא הגדרות ברורות
2. **חלון תוכן קצר** — נשלחו רק 1,500 תווים ראשונים. החלטות דקלרטיביות לפעמים מתחילות בהקדמה אופרטיבית
3. **ברירת מחדל אופרטיבית** — כשהתשובה לא ברורה, הקוד החזיר "אופרטיבית" — מה שהגדיל את ההטיה

**פתרון שיושם:**
- שיפור הפרומפט עם הגדרות ו-2 דוגמאות לכל סוג
- הגדלת חלון התוכן ל-3,000 תווים
- שינוי ברירת מחדל ל-"לא ברור" (דגל לבדיקה ידנית)
- ✅ **תיקון AI (31.01.2026):** 1,322 רשומות עם keyword mismatch סווגו מחדש עם pre-filtering חכם. ההטיה ירדה מ-84% ל-~79%.

**לקח כללי:** כל שדה AI חייב פרומפט עם דוגמאות + אסור שברירת המחדל תטה לכיוון אחד.

---

### 1.5 תגיות מדיניות לא תואמות תוכן — Policy Tag Relevance (חומרה: גבוהה) ✅ שופר

**מה קרה:** 62% מהרשומות (לאחר כיול) עם לפחות תג מדיניות אחד שאין לו אף מילת מפתח בתוכן.

**ניתוח:**
- חלק מהמקרים הם **תיוגים שגויים** — ה-AI תייג תחום שלא רלוונטי (למשל "ביטחון פנים" להחלטה על סגירת ערוץ שידור זר)
- חלק הם **false positives** — מילון המילים לא מכסה מספיק צורות מורפולוגיות בעברית
- חלק הם **מקרי גבול** — ההחלטה קשורה לתחום בעקיפין אבל לא מזכירה מילות מפתח ישירות

**פתרונות שיושמו:**
- הוספת מילות מפתח לתגים עם שיעור החמצה גבוה (ביטחון פנים, חקיקה, משפטים, תחבורה, תקשורת, חוץ, תקציב)
- יישום `_word_in_text()` — בדיקת מילות מפתח עם טיפול בתחיליות עבריות (ב,ל,מ,ה,ו,כ,ש)
- סינון תוכן פגום (Cloudflare, קצר מדי)
- ✅ **שיפור אלגוריתם + AI (31.01.2026):** שכתוב `_word_in_text()` ל-6 שכבות + הרחבת מילוני מילות מפתח + AI re-tag של 918 רשומות "תרבות וספורט" + 9 רשומות "שונות". **שיעור ירד מ-62% ל-34.3%.**

**לקח כללי:**
- מילוני מילות מפתח חייבים להיות מספיק רחבים אבל לא גנריים מדי
- עברית דורשת טיפול מורפולוגי — "ביטחון" שונה מ-"בביטחון" ב-substring match רגיל
- אסור להוסיף מילים גנריות שמופיעות בכל החלטה (למשל "תקציב", "מיליון", "ש"ח" לתג תקציבי)

---

### 1.6 קומבינציות ברירת מחדל של גופים ממשלתיים — Body Default Patterns (חומרה: קריטית) ✅ תוקן

**מה קרה:** ב-AI Tagger, כשהמודל לא בטוח לאיזה גוף ממשלתי לשייך החלטה, הוא מחזיר קומבינציות "ברירת מחדל" — תמיד אותם משרדים. סריקה על ~15,000 רשומות (60% מכל שנה, דגימה שכבתית) גילתה 3 דפוסים:

| קומבינציה | סה"כ | לא קשור לתוכן | אחוז |
|-----------|-------|---------------|------|
| "משרד הרווחה" (לבד) | ~2,815 (כל ה-DB) | ~2,642 | 95.3% |
| "משרד הרווחה; משרד הבריאות; משרד העבודה" | ~1,499 | ~1,344 | 93.3% |
| "משרד הרווחה; משרד החינוך; משרד הבריאות" | ~447 | ~415 | 92.9% |

**אימות:** קראנו את תוכן 10 החלטות אקראיות מכל קומבינציה — בכולן אין שום קשר לגופים שתויגו. למשל: החלטה על ביטחון לאומי תויגה "משרד הרווחה; משרד החינוך; משרד הבריאות".

**שורשי הבעיה:**
1. **ברירת מחדל של ה-AI** — כשהפרומפט לא ברור מספיק וה-AI לא מזהה גוף ספציפי, הוא נוטה להחזיר תמיד "משרד הרווחה" כבחירת default
2. **שלושיות חוזרות** — בגלל ש-3-step validation מכפיף לרשימה מורשית, הרבה תשובות "רנדומליות" מתמפות לאותם 2-3 שילובים
3. **חוסר הוראה לא-לתייג** — הפרומפט לא הורה ל-AI להחזיר מחרוזת ריקה כשאין גוף ברור

**סורק:** `check_body_default_patterns` — בודק אם מילות מפתח של הגופים המתויגים מופיעות בפועל בתוכן

**תיקון שבוצע (31.01.2026):**
- ✅ **אלגוריתמי:** `fix_government_bodies` — הסיר 12,457 גופים שלא הופיעו בטקסט ולא היו רלוונטיים סמנטית ($0)
- ✅ **AI:** `fix_government_bodies_ai` — 483 רשומות עם default combos תויגו מחדש עם `generate_government_body_tags_validated()` (96.8% תויגו לגופים שונים)
- **תוצאה:** body-default ירד מ-93 ל-**0** בסריקת דגימה (100%- ירידה)

---

### 1.7 תג מדיניות ברירת מחדל — Policy Default "תרבות וספורט" (חומרה: קריטית) ✅ תוקן

**מה קרה:** ~1,033 רשומות תויגו "תרבות וספורט" כתג מדיניות יחיד, אבל ~89% מהן לא קשורות לתרבות או ספורט כלל.

**היקף (מסריקה על 15K):** 617 רשומות עם "תרבות וספורט" כתג יחיד, מתוכן 550 (89.1%) בלי אף מילת מפתח רלוונטית בתוכן.

**שורשי הבעיה:** דומה לבעיית body default — ה-AI בוחר "תרבות וספורט" כברירת מחדל כשלא מזהה תחום ברור.

**סורק:** `check_policy_default_patterns` — בודק אם מילות מפתח של "תרבות וספורט" מופיעות בתוכן

**תיקון שבוצע (31.01.2026):** ✅ `fix_policy_tags_defaults` — 918 רשומות תויגו מחדש עם `generate_policy_area_tags_strict()`. 99.2% אכן קיבלו תגים שונים מ-"תרבות וספורט". **תוצאה:** policy-default ירד מ-27 ל-**0** בסריקת דגימה.

---

### 1.8 ערכי אופרטיביות פגומים — Operativity Validity (חומרה: גבוהה) ✅ תוקן

**מה קרה:** 17 רשומות (מלוא ה-DB) עם ערכי אופרטיביות פגומים — שגיאות כתיב, בעיות קידוד, וזבל.

**דוגמאות:**
- "דלקרטיבית" → "דקלרטיבית" (החלפת אותיות)
- "אופרייטיבית" → "אופרטיבית" (שגיאת כתיב)
- "דקלרטיווית" → "דקלרטיבית" (וו מיותרת)
- ערכים עם `�` (קידוד פגום)

**סורק:** `check_operativity_validity` — מזהה ערכים שאינם "אופרטיבית", "דקלרטיבית" או ריק

**תיקון שבוצע (31.01.2026):** ✅ `fix_operativity_typos` — 17 רשומות תוקנו אלגוריתמית דרך `OPERATIVITY_TYPO_MAP` (**$0, בלי AI**). **תוצאה:** operativity-validity ירד מ-5 ל-**0** בסריקת דגימה.

---

### 1.9 גופים ממשלתיים לא בטקסט — Government Body Hallucination (חומרה: בינונית) ✅ תוקן

**מה קרה:** ב-87% מהרשומות (לפני כיול), לפחות גוף ממשלתי אחד שתויג לא נמצא בטקסט.

**ניתוח מעמיק (לאחר כיול):**
- **126 רשומות (25.3%)** — כל הגופים חסרים (חומרה גבוהה — חשד להזיה)
- **123 רשומות (24.6%)** — חלק מהגופים חסרים (חומרה נמוכה — ייתכן שרלוונטי בעקיפין)
- **250 רשומות (50.1%)** — כל הגופים נמצאו בטקסט

**שורשי הבעיה:**
1. **נוסח שרים** — החלטות ממשלתיות מזכירות "שר הביטחון" ולא "משרד הביטחון". מילון הקיצורים לא כלל את דפוס "שר ה..."
2. **ייחוס סמנטי** — ה-AI תייג גופים על סמך רלוונטיות תוכנית, גם אם הם לא מוזכרים מילולית בהחלטה
3. **תוכן פגום** — חלק מהרשומות (Cloudflare/קצר) לא הכילו את הטקסט האמיתי

**פתרונות שיושמו:**
- הוספת דפוס "שר/שרת ה..." לכל משרד (למשל "שר הביטחון", "שרת החינוך")
- הוספת קיצורים חסרים (למשל "המפכ"ל", "מועצה להשכלה גבוהה", "נציב שירות המדינה")
- שינוי מבנה הבדיקה: חומרה גבוהה רק כשכל הגופים חסרים, חומרה נמוכה כשחלק חסרים
- בדיקה גם ב-summary ו-title, לא רק ב-content
- סינון Cloudflare content
- ✅ **הסקה סמנטית (31.01.2026):** פיתוח `_is_body_semantically_relevant()` — גוף ממשלתי נשמר אם תגי המדיניות שלו רלוונטיים, גם אם לא מוזכר מילולית
- ✅ **תיקון אלגוריתמי:** `fix_government_bodies` — הסיר 12,457 גופים לא-רלוונטיים
- ✅ **תיקון AI:** `fix_government_bodies_ai` — 483 רשומות עם default combos תויגו מחדש
- **תוצאה:** gov-body-hallucination ירד מ-506 ל-**2** בסריקת דגימה (**-99.6%**)

**לקח כללי:**
- מילון קיצורים צריך להכיל גם:
  - שם רשמי ("משרד הביטחון")
  - קיצור ("מש"הב")
  - תבנית שר/שרה ("שר הביטחון", "שרת הביטחון")
  - שם תחום בודד ("הביטחון")
- ייחוס סמנטי (AI תייג גוף רלוונטי שלא מוזכר) הוא לגיטימי — לא בהכרח שגיאה
- הסקה סמנטית דרך TAG_BODY_MAP מונעת הסרה של גופים לגיטימיים שלא מוזכרים מילולית

---

### 1.10 סיכום לא תואם תגים — Summary vs Tags (חומרה: נמוכה — אינפורמטיבי)

**מה קרה:** 88% מהרשומות עם לפחות תג אחד שאין לו מילות מפתח בסיכום.

**למה זה לא באמת בעיה:** סיכומים הם 1-2 משפטים (60-150 תווים). אין סיכוי סביר שסיכום כזה קצר יכיל מילות מפתח ספציפיות לכל תג מדיניות. הבדיקה הזו מידע אינפורמטיבי בלבד.

**פתרון:** הוספת סף מינימלי של 60 תווים לסיכום + בדיקה ראשונה אם שם התג עצמו מופיע.

**לקח:** לא כל cross-check הגיוני בתיאוריה עובד בפועל. סיכומים קצרים פשוט לא יכילו מספיק אינפורמציה.

---

### 1.11 עקביות תג-גוף — Tag-Body Consistency (חומרה: נמוכה)

**מה קרה:** 107% issue rate (יותר בעיות מרשומות) — כי כל שילוב תג-גוף שלא תואם נספר בנפרד.

**למה זה לא בהכרח בעיה:** החלטות ממשלתיות הן לרוב **חוצות-משרדים**. החלטה על "חינוך" יכולה להיות עם "משרד האוצר" בגלל תקציב. זה לגיטימי לחלוטין.

**פתרון:** צמצום הבדיקה רק לרשומות עם **תג מדיניות יחיד** (שם הציפייה להתאמה ברורה חזקה יותר). ירידה מ-538 ל-164.

**לקח:** בדיקות cross-field חייבות להתחשב בטבע הנתונים. החלטות ממשלתיות הן inherently cross-domain.

---

### 1.12 כותרת לא תואמת תוכן — Title vs Content (חומרה: בינונית)

**מה קרה:** 27 רשומות (5.4%) עם 0% התאמה בין מילות הכותרת לתוכן.

**שורשי הבעיה:**
1. **תוכן Cloudflare** — רוב הרשומות עם 0% התאמה הכילו טקסט Cloudflare ולא את ההחלטה
2. **מורפולוגיה עברית** — מילת כותרת "בביטחון" לא נמצאה בתוכן שמכיל "ביטחון"

**פתרון:** סינון Cloudflare + פונקציית `_word_in_text()` שמטפלת בתחיליות עבריות. ירידה מ-27 ל-1.

**לקח:** עברית דורשת prefix-aware matching. פונקציית substring רגילה לא מספיקה.

---

### 1.13 מיקומים "הזויים" — Location Hallucination (חומרה: בינונית) ✅ תוקן

**מה קרה:** 2 מתוך 62 רשומות עם מיקומים (3.2%) — מיקום שתויג לא נמצא בטקסט.

**דוגמאות:**
- "מזרח ירושלים" — לא מופיע בטקסט אבל "ירושלים" כן
- "ממשלה" — סווג בטעות כמיקום

**תיקון שבוצע (31.01.2026):** ✅ `fix_locations` — הסיר 390 מיקומים שלא הופיעו בטקסט ($0, אלגוריתמי). **תוצאה:** location-hallucination ירד מ-23 ל-**0** בסריקת דגימה.

**לקח:** `generate_location_tags()` צריך ולידציה: `if loc in decision_content`.

---

## 2. בעיות שלא נמצאו (הכל תקין)

| בדיקה | תוצאה | משמעות |
|--------|--------|--------|
| **Policy fallback (שונות)** | 0% | אין החלטות עם "שונות" כתג יחיד — מיגרציית התגים הצליחה |
| **Committee-tag consistency** | 0% | שם ועדה תמיד תואם את תג המדיניות |
| **Date validity** | 0% | כל התאריכים בטווח תקין (1948-היום) |
| **Date vs government** | 0% | אין החלטות של ממשלה 37 לפני 29.12.2022 |
| **Content completeness** | 0% | אין תוכן שנקטע באמצע משפט |

---

## 3. לקחים כלליים למערכת QA

### 3.1 מורפולוגיה עברית
- **בעיה:** substring match רגיל נכשל עם תחיליות (ב,ל,מ,ה,ו,כ,ש) וסיומות (ים, ות)
- **פתרון (v2, ינואר 2026):** `_word_in_text()` עם 6 שכבות התאמה:
  1. exact match — מילה כמו שהיא
  2. prefix-stripped — הפשטה כפולה של תחיליות (ש+ב → ביטחון)
  3. suffix-stripped — הסרת סיומות (ים, ות, ן, ית, יים, יות)
  4. stem — הפשטה משולבת (תחילית + סיומת)
  5. prefix-added — בדיקה עם כל תחילית אפשרית (ב+מילה, ל+מילה...)
  6. combined — prefix-added על מילה מופשטת
- **מה חסר:** טיפול בצורות פועל (יפעל/פעל/פועל) ובשמות פעולה
- **המלצה לעתיד:** שקילת שימוש בספריית lemmatization עברית (כמו `hebrew_tokenizer`) לדיוק גבוה יותר

### 3.2 מילוני מילות מפתח
- **עיקרון:** כל תג מדיניות חייב מילון עם 10-25 מילות מפתח ספציפיות
- **מה לא לכלול:** מילים גנריות שמופיעות בהחלטות רבות (תקציב, מיליון, ש"ח, פעולה, החלטה, ממשלה)
- **מה כן:** שמות מוסדות, מונחים מקצועיים, פעלים ייחודיים לתחום
- **דוגמה טובה:** "בינוי ושיכון" → ["בינוי", "שיכון", "בנייה", "דירות", "מגורים", "יחידות דיור", "פינוי בינוי"]
- **דוגמה רעה:** "תקציב" → ["תקציב", "מיליון", "ש"ח"] — מופיע בכל החלטה שנייה

### 3.3 Severity classification
- **HIGH:** בעיה שמשפיעה על נכונות הנתונים (תוכן פגום, תיוג שגוי, הזיה)
- **MEDIUM:** בעיה אפשרית שדורשת בדיקה (חוסר התאמה שייתכן לגיטימי)
- **LOW:** אינפורמטיבי בלבד (cross-field שלא בהכרח מעיד על בעיה)

### 3.4 False positive management
- **בעיה:** בדיקות ראשוניות הראו שיעורי בעיה של 87-98% — רובן false positives
- **סיבות:** מילונים לא מספיק רחבים, חוסר טיפול מורפולוגי, תוכן Cloudflare שלא סונן
- **פתרון:** כיול iterativi — הרצת scan, ניתוח דגימות, הרחבת מילונים, הרצה חוזרת
- **עיקרון:** בדיקת QA שמחזירה >50% issues כנראה מוגדרת לא נכון

### 3.5 Cross-field checks
- **עיקרון:** בדיקות עקביות בין שדות חייבות להתחשב בטבע הנתונים
- **דוגמה:** tag-body consistency — החלטות חוצות-משרדים הן לגיטימיות → בדוק רק single-tag records
- **דוגמה:** summary-vs-tags — סיכומים קצרים מדי לבדיקת keywords → הפוך לאינפורמטיבי בלבד

---

## 4. סיכום מספרי

### 4.1 סריקה על 500 רשומות (אחרי כיול — Phase 1-3, לפני תיקונים)

| בדיקה | ממצאים | שיעור | חומרה | פעולה נדרשת |
|--------|--------|--------|--------|-------------|
| Content quality (Cloudflare) | 24 | 4.8% | HIGH | Re-scrape |
| Operativity bias | 84% אופרטיבית | — | HIGH | Re-classify (AI) |
| Policy tag relevance | 296 | 62.3% | HIGH | Re-tag flagged (AI) |
| Gov body hallucination (all missing) | 126 records | 25.3% | HIGH | Re-tag (AI) |
| Gov body hallucination (some missing) | 123 records | 24.6% | LOW | Review |
| Operativity vs content | 41 | 8.2% | MEDIUM | Re-classify (AI) |
| Tag-body consistency | 164 | 32.8% | LOW | Informational |
| Summary vs tags | 440 | 88.4% | LOW | Informational |
| Content quality (short/nav) | 3 | 0.6% | MEDIUM | Re-scrape |
| Tag consistency (שונות as body) | 3 | 0.6% | MEDIUM | Fix tag |
| Location hallucination | 2 | 3.2% | MEDIUM | Remove (algorithm) |
| Location vs body | 21 | 33.9% | LOW | Informational |
| Title vs content | 1 | 0.2% | MEDIUM | Investigate |

### 4.2 סריקה על ~15,000 רשומות (60% שכבתי — Phase 4: דפוסי ברירת מחדל)

| בדיקה | נסרקו | ממצאים (HIGH) | שיעור | פעולה נדרשת |
|--------|--------|--------------|--------|-------------|
| Body default: "משרד הרווחה" (לבד) | 1,673 | 1,594 | 95.3% | AI re-tag (`government-bodies-ai`) |
| Body default: רווחה+בריאות+עבודה | 871 | 813 | 93.3% | AI re-tag (`government-bodies-ai`) |
| Body default: רווחה+חינוך+בריאות | 253 | 235 | 92.9% | AI re-tag (`government-bodies-ai`) |
| Policy default: "תרבות וספורט" | 617 | 550 | 89.1% | AI re-tag (`policy-tags-defaults`) |
| Operativity validity (typos) | 14,996 | 6 | 0.04% | Algorithmic fix (`operativity-typos`, $0) |

**אקסטרפולציה למלוא ה-DB (~25K):**

| בעיה | הערכה | עלות תיקון |
|------|-------|-----------|
| Body default combos (3 דפוסים) | ~4,761 רשומות | ~$3-8 (AI) |
| Policy default "תרבות וספורט" | ~1,033 רשומות | ~$1-3 (AI) |
| Operativity typos | ~17 רשומות | **$0** (algorithmic) |

### 4.3 שיפורי אלגוריתם ותיקונים אלגוריתמיים (31 בינואר 2026)

**שלב 1: שיפור אלגוריתם הזיהוי**

שיפורים שבוצעו ב-qa.py:
1. `_word_in_text()` — שכתוב מלא עם 6 שכבות התאמה: exact → prefix-stripped → suffix-stripped → stem → prefix-added → combined
2. `_strip_hebrew_prefix()` — הפשטה כפולה (למשל: "שבביטחון" → "בביטחון" → "ביטחון")
3. `_strip_hebrew_suffix()` — פונקציה חדשה לטיפול בסיומות (ים, ות, ן, ית, יים, יות)
4. `_is_body_in_text()` — שימוש ב-`_word_in_text()` + תבנית "המשרד ל..."
5. `_is_body_semantically_relevant()` — שכבת הסקה סמנטית חדשה: בודק אם גוף ממשלתי רלוונטי לתגי המדיניות שהוקצו (דרך TAG_BODY_MAP reverse lookup) או שיש evidence של מילות מפתח בתוכן (≥2 hits)
6. `BODY_TO_TAGS_MAP` — מילון הפוך של TAG_BODY_MAP לשימוש בהסקה סמנטית
7. `POLICY_TAG_KEYWORDS` — הרחבה של +42 מילות מפתח ל-7 תגים עם כיסוי נמוך
8. `BODY_ABBREVIATIONS` — הרחבה של +13 שמות היסטוריים ל-7 גופים

**השוואת תוצאות (482 רשומות, דגימה שכבתית, seed=42):**

| מדד | Baseline | אחרי שיפור | אחרי תיקון | שינוי כולל |
|-----|----------|-----------|-----------|-----------|
| סה"כ ממצאים | 1,489 | 1,369 | **673** | **-55%** |
| חומרה HIGH | 829 | 723 | **217** | **-74%** |
| location-hallucination | 23 | 9 | **0** | **-100%** |
| gov-body-hallucination | 506 | 487 | **0** | **-100%** |
| operativity-validity | 5 | 5 | **0** | **-100%** |
| body-default | 93 | 93 | **3** | **-97%** |
| policy-relevance | 257 | 175 | 183 | **-29%** |
| tag-body | 164 | 164 | 109 | **-34%** |
| title-vs-content | 6 | 4 | 3 | **-50%** |

**שלב 2: תיקונים אלגוריתמיים ($0, בלי AI)**

| תיקון | רשומות שתוקנו | שגיאות | תאריך |
|-------|--------------|--------|-------|
| `operativity-typos` | 17 | 0 | 31.01.2026 |
| `locations` | 390 | 0 | 31.01.2026 |
| `government-bodies` | 12,457 | 0 | 31.01.2026 |

**לקחים משלב האלגוריתם:**
- הפשטה כפולה של תחיליות הכרחית בעברית — "שבביטחון" דורש 2 הפשטות
- הסקה סמנטית חיונית — גוף ממשלתי לא חייב להופיע מילולית אם תחום המדיניות שלו רלוונטי
- תיקון אלגוריתמי של government-bodies הוא האגרסיבי ביותר (49.8% מכל הרשומות) — ההסקה הסמנטית מנעה הסרה של ~620 רשומות שהיו לגיטימיות

### 4.4 תיקוני AI (31 בינואר 2026)

| תיקון | רשומות שתוקנו | שגיאות | אלגוריתם |
|-------|--------------|--------|----------|
| `policy-tags` (שונות בלבד) | 9 | 0 | `generate_policy_area_tags_strict()` |
| `policy-tags-defaults` (תרבות וספורט) | 918 | 0 | `generate_policy_area_tags_strict()` |
| `government-bodies-ai` (default combos) | 483 | 0 | `generate_government_body_tags_validated()` |
| `summaries` (קצרים/ארוכים) | 590 | 1 | `generate_summary()` |
| `operativity` (keyword mismatch) | 1,322 | 0 | GPT-3.5 + keyword evidence |
| `cloudflare` (re-scrape + AI) | 24 | 0 | Selenium re-scrape + `process_decision_with_ai()` |

**סה"כ רשומות שתוקנו ע"י AI:** 3,346 (כולל 24 cloudflare re-scrape)

**שיפור ב-operativity fixer:** הוספת pre-filtering ב-`bin/qa.py` — במקום לשלוח 25K רשומות ל-AI, מסנן רק רשומות שבהן keyword evidence סותר את הסיווג הנוכחי (1,379 במקום 25,019). חיסכון של ~95% בעלויות AI.

**תוצאות סריקה סופית (482 רשומות, דגימה שכבתית, seed=42):**

| מדד | Baseline | אחרי אלגוריתם | אחרי AI | **סופי (+ CF re-scrape)** | שינוי כולל |
|-----|----------|---------------|---------|--------------------------|-----------|
| סה"כ ממצאים | 1,489 | 673 | 585 | **585** | **-61%** |
| חומרה HIGH | 829 | 217 | 164 | **164** | **-80%** |
| content-quality (CF) | 24 | 24 | 24 | **4** (short only) | **-83%** |
| operativity-vs-content | 41 | 26 | 1 | **1** | **-97.6%** |
| body-default | 93 | 3 | 0 | **0** | **-100%** |
| policy-default | 27 | 27 | 0 | **0** | **-100%** |
| summary-quality | 8 | 8 | 0 | **0** | **-100%** |
| gov-body-hallucination | 506 | 0 | 2 | **2** | **-99.6%** |
| policy-relevance | 257 | 183 | 164 | **164** | **-36%** |
| operativity-validity | 5 | 5 | 0 | **0** | **-100%** |
| location-hallucination | 23 | 9 | 0 | **0** | **-100%** |

**הערה:** הסריקה הסופית (אחרי CF re-scrape) מראה אותם 585 ממצאים כמו לפני ה-re-scrape, כי 24 רשומות ה-Cloudflare היו מסוננות מכל הבדיקות מלכתחילה. השיפור העיקרי: `content-quality` ירד מ-24 ל-4 (נשארו רק 4 רשומות עם תוכן קצר מ-100 תווים).

**לקחים משלב ה-AI:**
- Pre-filtering חיוני — שליחת כל ה-DB ל-AI היא בזבוז. יש לסנן רק רשומות עם evidence לבעיה
- `policy-tags-defaults` הוכיח שהבעיה אמיתית: 99.2% מרשומות "תרבות וספורט" אכן תויגו מחדש לתגים אחרים
- `government-bodies-ai` הוכיח ש-"משרד הרווחה" כ-default: 96.8% תויגו מחדש
- ה-summaries fixer עם pre-filtering חכם (רק short/long/identical) מנע 98% מקריאות AI מיותרות

---

## 5. המלצות לאוטומציה עתידית

### 5.1 תיקונים אלגוריתמיים — ✅ הושלמו ($0, בלי AI)
- **`operativity-typos`** — ✅ תוקנו 17 שגיאות כתיב/קידוד פגום (31.01.2026)
- **`locations`** — ✅ הוסרו 390 מיקומים שלא הופיעו בטקסט (31.01.2026)
- **`government-bodies`** — ✅ הוסרו 12,457 גופים שלא הופיעו בטקסט ולא היו רלוונטיים סמנטית (31.01.2026)

### 5.2 תיקוני AI — ✅ הושלמו (31.01.2026)
- **`policy-tags`** — ✅ 9 רשומות עם "שונות" → תגים חדשים
- **`policy-tags-defaults`** — ✅ 918 רשומות עם "תרבות וספורט" → תגים חדשים
- **`government-bodies-ai`** — ✅ 483 רשומות עם default combos → גופים נכונים
- **`summaries`** — ✅ 590 סיכומים קצרים/ארוכים → נוצרו מחדש
- **`operativity`** — ✅ 1,322 רשומות עם keyword mismatch → סיווג מחדש

### 5.3 סדר ביצוע התיקונים — ✅ הושלם (31.01.2026)

התיקונים בוצעו בסדר הבא (אלגוריתמי קודם, אח"כ AI):

```
✅ 1. fix operativity-typos execute       # $0, 17 רשומות
✅ 2. fix locations execute               # $0, 390 רשומות
✅ 3. fix government-bodies execute       # $0, 12,457 רשומות
✅ 4. fix policy-tags execute             # AI, 9 רשומות (שונות)
✅ 5. fix policy-tags-defaults execute    # AI, 918 רשומות (תרבות וספורט)
✅ 6. fix government-bodies-ai execute    # AI, 483 רשומות (default combos)
✅ 7. fix summaries execute               # AI, 590 רשומות (קצרים/ארוכים)
✅ 8. fix operativity execute             # AI, 1,322 רשומות (keyword mismatch)
✅ 9. fix cloudflare execute              # Re-scrape + AI, 24 רשומות
```

**סה"כ:** 16,210 תיקונים (3,346 AI + 12,864 אלגוריתמי)

### 5.4 בדיקות שדורשות human review
- **Tag-body consistency** — דורש הבנת הקשר (האם cross-ministry לגיטימי?)
- **Policy relevance borderline** — records עם low relevance (>0 but <15% keywords) דורשים שיקול דעת

### 5.5 Pipeline integration roadmap

**שלב 1 (מיושם):** `validate_decision_inline()` — warnings בלוג, לא חוסם הכנסה
**שלב 2 (מומלץ):** חסימת Cloudflare content — אם content מכיל patterns של Cloudflare, retry או דלג
**שלב 3 (מומלץ):** חסימת hallucinations — אם location לא בטקסט, הסר אוטומטית לפני הכנסה ל-DB
**שלב 4 (מומלץ):** body default detection — אם body combo שווה לאחד מ-`SUSPICIOUS_BODY_COMBOS`, retry עם פרומפט משופר
**שלב 5 (מומלץ):** policy default detection — אם תג יחיד = "תרבות וספורט" ואין evidence, retry

**הערה:** שלבים 2-5 רלוונטיים לרשומות **חדשות** שנכנסות דרך הפייפליין. התיקונים שבוצעו (5.1-5.3) כיסו את ה-DB הקיים.

---

## 6. טכניקות QA שפותחו

### 6.1 Hebrew morphology-aware matching (v2)
```python
def _word_in_text(word, text):
    # 6-tier matching:
    # 1. Exact: word in text
    # 2. Prefix-stripped (×2): "שבביטחון" → "בביטחון" → "ביטחון"
    # 3. Suffix-stripped: "חקלאים" → "חקלא"
    # 4. Stem (prefix+suffix): "שבחקלאים" → "חקלא"
    # 5. Prefix-added: check "ב"+word, "ל"+word, etc.
    # 6. Combined: prefix-added on stripped word
```

### 6.8 Semantic body inference
```python
BODY_TO_TAGS_MAP = {reverse of TAG_BODY_MAP}

def _is_body_semantically_relevant(body, policy_tags, content):
    # 1. Check if body's expected tags overlap with assigned policy tags
    # 2. Fallback: check if body's expected tag keywords appear in content (≥2 hits)
    # Preserves bodies that are semantically relevant even if not literally mentioned
```

### 6.2 Government body abbreviation map
```python
BODY_ABBREVIATIONS = {
    "משרד הביטחון": ["הביטחון", "משרד הביטחון", "מש\"הב", "שר הביטחון", "שרת הביטחון"],
    # 40+ entries with 3-5 variants each
}
```

### 6.3 Policy keyword dictionaries
```python
POLICY_TAG_KEYWORDS = {
    "ביטחון פנים": ["ביטחון פנים", "טרור", "פשיעה", ...],
    # 40 tags with 10-25 keywords each
    # Precision rules: no generic words, no cross-domain terms
}
```

### 6.4 Severity-based triage
- ALL bodies missing → HIGH (likely hallucination)
- SOME bodies missing → LOW (likely implicit reference)
- Single-tag record with mismatched body → check (clear domain)
- Multi-tag record with mismatched body → skip (cross-ministry is normal)

### 6.5 Default/fallback pattern detection
```python
SUSPICIOUS_BODY_COMBOS = [
    "משרד הרווחה",                                    # sole — 95.3% unrelated
    "משרד הרווחה; משרד הבריאות; משרד העבודה",          # trio — 93.3% unrelated
    "משרד הרווחה; משרד החינוך; משרד הבריאות",          # trio — 92.9% hallucination
]
SUSPICIOUS_POLICY_TAGS = ["תרבות וספורט"]  # sole — 89.1% unrelated
```
**עקרון:** כשה-AI לא בטוח, הוא נוטה לחזור על אותן תשובות. זיהוי הדפוס הזה מאפשר re-tag ממוקד.

### 6.6 Operativity typo map
```python
OPERATIVITY_TYPO_MAP = {
    "דלקרטיבית": "דקלרטיבית",    # letter swap
    "אופרייטיבית": "אופרטיבית",   # extra yod
    # 11 known typos → 2 correct values
}
```
**עקרון:** תיקון אלגוריתמי ל-$0 — ערכים עם קידוד פגום מתאפסים לריק (ל-AI re-classify מאוחר יותר).

### 6.7 Stratified sampling
```python
fetch_records_stratified(sample_percent_per_year=10, seed=42)
# Fetches all records, groups by year, samples N% from each
# Ensures representation across all 34 years (1993-2026)
```
**עקרון:** דגימה שכבתית מונעת הטיה לכיוון רשומות חדשות. `seed` מאפשר שחזור.

---

## 7. קבצים רלוונטיים

| קובץ | תפקיד |
|-------|--------|
| [src/gov_scraper/processors/qa.py](src/gov_scraper/processors/qa.py) | סורקים, מתקנים, מילונים |
| [bin/qa.py](bin/qa.py) | CLI לסריקה ותיקון |
| [src/gov_scraper/processors/ai.py](src/gov_scraper/processors/ai.py) | פרומפטים (operativity משופר) |
| [bin/sync.py](bin/sync.py) | שילוב inline validation |
| [data/qa_reports/](data/qa_reports/) | דוחות סריקה (JSON) |
